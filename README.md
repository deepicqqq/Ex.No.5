# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS AND EXPLAIN WITH VARIOUS TEST SCENARIOS
## NAME: DEEPIKA P
## REGISTER NO: 212223240024

# Aim: 
To test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios.  Analyze the quality, accuracy, and depth of the generated responses 

## AI Tools Required:

ChatGPT (or any similar AI text generation tool)

## Explanation:
Prompt Types

Naïve Prompt – Broad, vague, or unstructured request.

Basic Prompt – Clear, detailed, and structured prompt with context and specific instructions.
## Test Scenarios & Prompts
| **Scenario**                    | **Naïve Prompt**         | **Basic Prompt**                                                                                                                                        | **Response Quality (Naïve)**                                        | **Response Quality (Basic)**                                               |
| ------------------------------- | ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------- | -------------------------------------------------------------------------- |
| **1. Creative Story**           | *“Tell me a story.”*     | *“Write a short story (200–250 words) about a child who discovers a hidden magical library. Use descriptive language and include a surprising ending.”* | Very generic story, lacks detail and depth.                         | Detailed, imaginative, structured story with rich descriptions.            |
| **2. Factual Question**         | *“Who is Newton?”*       | *“Explain who Sir Isaac Newton was, highlighting his key contributions to physics and mathematics, in 5–6 sentences.”*                                  | Short, surface-level answer (“scientist, laws of motion, gravity”). | Clear, concise, structured explanation with key contributions highlighted. |
| **3. Summarization**            | *“Summarize AI.”*        | *“Summarize Artificial Intelligence in 4–5 sentences, focusing on its definition, applications, and impact on daily life.”*                             | Too broad, incomplete, lacks focus.                                 | Precise summary with definition, applications, and impact.                 |
| **4. Advice / Recommendation**  | *“Give me some advice.”* | *“Provide three pieces of advice for a college student on how to balance studies, skill development, and personal well-being.”*                         | Random, unfocused advice (sometimes unrelated).                     | Relevant, structured, practical advice with clear categories.              |
| **5. Explanation of a Concept** | *“Explain blockchain.”*  | *“Explain blockchain technology in simple terms for a beginner. Include its definition, how it works, and one real-life application in 5–6 sentences.”* | Vague explanation with jargon.                                      | Beginner-friendly, accurate, with real-life application.                   |

## Analysis of Results:

1.Quality – Basic prompts consistently produced richer, more structured, and coherent responses compared to naïve prompts.

2.Accuracy – Basic prompts improved factual correctness and ensured focus on required details.

3.Depth – Basic prompts guided ChatGPT to provide more thorough and context-aware answers.

4.Scenarios where naïve prompts worked well – Simple factual recall tasks (e.g., “Who is Newton?”) still gave acceptable responses, though basic prompts improved clarity.

## Summary of Findings:

Prompt clarity directly impacts the quality, accuracy, and depth of ChatGPT responses.

Naïve prompts lead to vague or generic answers, while basic prompts guide the model toward more focused and useful outputs.

For creative, advisory, and explanatory tasks, structured prompts make a significant difference.

For basic factual queries, naïve prompts can sometimes suffice but still lack depth.

## Result:

The experiment was successfully executed. Comparative analysis proves that well-structured prompts consistently yield better AI-generated outputs in terms of quality, accuracy, and depth.

